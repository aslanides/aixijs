{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "import matplotlib as mpl\n",
    "label_size = 20\n",
    "mpl.rcParams['xtick.labelsize'] = label_size\n",
    "mpl.rcParams['ytick.labelsize'] = label_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results for all environments except reward corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent_labels = {'BayesAgent':(r'AI$\\xi$','red'),\n",
    "          'MC-AIXI':('MC-AIXI','red'),\n",
    "          'MC-AIMU':('MC-AIMU','blue'),\n",
    "          'MDL Agent':('MDL','blue'),\n",
    "          'MC-AIXI-Dirichlet':('MC-AIXI-Dirichlet','blue'),\n",
    "          'Knowledge-seeking agent':('Kullback-Leibler','blue'),\n",
    "          'KullbackLeiblerKSA':('Kullback-Leibler','blue'),\n",
    "          'ShannonKSA':('Shannon','green'),\n",
    "          'SquareKSA':('Square','red'),\n",
    "          'Shannon KSA':('Shannon','orange'),\n",
    "          'Square KSA':('Square','red'),\n",
    "          'ThompsonAgent':('Thompson Sampling','blue'),\n",
    "          'Thompson Sampling':('Thompson Sampling','blue'),\n",
    "          'QLearn':('Q-Learning','black'),\n",
    "          'Q-Learning':('Q-Learning','black'),\n",
    "          'KSA-Dirichlet': ('Kullback-Leibler','blue'),\n",
    "          'Entropy-seeking agent': ('Shannon','orange'),\n",
    "          'Square KSA-Dirichlet': ('Square','red')}\n",
    "\n",
    "def plot_results(directory,\n",
    "                 filename='results-1',\n",
    "                 objective=None,\n",
    "                 outfile=None,\n",
    "                 show_optimal=False,\n",
    "                 show_variance=True,\n",
    "                 show_maxmin=False):\n",
    "    \n",
    "    # some cruft to add default labels\n",
    "    if not objective:\n",
    "        if 'ksa' in directory:\n",
    "            objective = 'explored'\n",
    "        else:\n",
    "            objective = 'rewards'\n",
    "    if objective == 'rewards':\n",
    "        y_axis = 'Average Reward'\n",
    "    elif objective == 'explored':\n",
    "        y_axis = 'Exploration (%)'\n",
    "    \n",
    "    file = open(directory + '/' + filename + '.json')\n",
    "    data = json.load(file)\n",
    "    file.close()\n",
    "\n",
    "    fig = plt.figure(figsize=(12,8),dpi=200)\n",
    "    # iterate over configs\n",
    "    for i,k in enumerate(data):\n",
    "        try:\n",
    "            d = data[k]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        cycles = d[0]['cycles']\n",
    "        runs = len(d)\n",
    "\n",
    "        A = np.zeros((cycles,runs))\n",
    "        for j in range(runs):\n",
    "            A[:,j] = np.array(d[j][objective][:cycles])\n",
    "        mu = np.mean(A,1)\n",
    "        sigma = np.std(A,1)\n",
    "        a = np.max(np.vstack((mu-sigma,np.min(A,1))),0)\n",
    "        b = np.min(np.vstack((mu+sigma,np.array(cycles*[100]))),0)\n",
    "\n",
    "        if k in agent_labels:\n",
    "            lab = agent_labels[k][0]\n",
    "\n",
    "        color = agent_labels[k][1]\n",
    "        alpha = 0.1\n",
    "        \n",
    "        if show_variance:\n",
    "            plt.plot(a,color=color,alpha=alpha)\n",
    "            plt.plot(b,color=color,alpha=alpha)\n",
    "            plt.fill_between(np.arange(cycles),a,b,alpha=alpha,color=color)\n",
    "        \n",
    "        if show_maxmin:\n",
    "            plt.plot(np.max(A,axis=1),color=color,linestyle='-.')\n",
    "            plt.plot(np.min(A,axis=1),color=color,linestyle='-.')\n",
    "        \n",
    "        plt.plot(mu,label=lab,color=color,lw=3)\n",
    "        \n",
    "\n",
    "    if objective=='rewards' and show_optimal:\n",
    "        # NOTE: hardcoded for optimal policy in one gridworld\n",
    "        xs = np.array(range(cycles))\n",
    "        ys = np.zeros(cycles)\n",
    "        ys[:11] = -1.\n",
    "        ys[11:] = 75.\n",
    "        ys = np.cumsum(ys)\n",
    "\n",
    "        ys[1:] /= xs[1:]\n",
    "        plt.plot(xs,ys,'k--',lw=3,label='Optimal')\n",
    "        \n",
    "    plt.xlabel('Cycles',fontsize=30)\n",
    "    plt.ylabel(y_axis,fontsize=30)\n",
    "    plt.legend(fontsize=25,loc='lower right')\n",
    "    plt.margins(0.01,0)\n",
    "    #plt.ylim([-1,100])\n",
    "       \n",
    "    if outfile:\n",
    "        plt.savefig(directory + '/' + outfile + '.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "plot_results('aixi-models','results-3',show_optimal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results for reward corruption "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rc_results(directory,\n",
    "                 filename='results',\n",
    "                 objective='rewards',\n",
    "                 outfile=None,\n",
    "                 show_variance=True,\n",
    "                 runs=5,\n",
    "                 cycles=1000000,\n",
    "                 color='red',\n",
    "                 ls='solid',\n",
    "                 label='Q-learning'):\n",
    "    if objective == 'rewards':\n",
    "        y_axis = 'Average Observed Reward'\n",
    "    elif objective == 'corrupt_rewards':\n",
    "        y_axis = 'Average Corrupt Reward'\n",
    "    elif objective == 'true_rewards':\n",
    "        y_axis = 'Average True Reward'\n",
    "    \n",
    "    A = np.zeros((cycles,runs))\n",
    "    for j in range(runs):\n",
    "        file = open(directory + '/' + filename + '-' + str(j+1) + '.json')\n",
    "        data = json.load(file)\n",
    "        file.close()\n",
    "        A[:,j] = np.array(data['Reward Corruption'][0][objective])\n",
    "\n",
    "    mu = np.mean(A,1)\n",
    "    sigma = np.std(A,1)\n",
    "    a = np.max(np.vstack((mu-sigma,np.min(A,1))),0)\n",
    "    b = np.min(np.vstack((mu+sigma,np.array(cycles*[100]))),0)\n",
    "\n",
    "    alpha = 0.1\n",
    "    if show_variance:\n",
    "        plt.plot(a, color=color, alpha=alpha, ls=ls)\n",
    "        plt.plot(b, color=color, alpha=alpha, ls=ls)\n",
    "        plt.fill_between(np.arange(cycles), a, b, alpha=alpha, color=color)\n",
    "\n",
    "    plt.plot(mu, color=color, label=label, lw=3, ls=ls)\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Cycles', fontsize=20)\n",
    "    \n",
    "    if outfile:\n",
    "        plt.savefig(directory + '/' + outfile + '.png', bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### plot results for different agents on the same plot\n",
    "runs=100 # number of runs\n",
    "goals='4' # number of goal tiles\n",
    "for rew in ('true_', ''):\n",
    "    plot_rc_results('reward-corruption/goals' + goals + '_qlearning', 'results', rew+'rewards', runs=runs, color='red')\n",
    "    plot_rc_results('reward-corruption/goals' + goals + '_softmax', 'results', rew+'rewards', runs=runs, color='orange', label='Softmax')\n",
    "    plot_rc_results('reward-corruption/goals' + goals + '_quantiliser_delta.2', 'results', rew+'rewards', runs=runs, color='black', label='Quantiliser (.2)')\n",
    "    plot_rc_results('reward-corruption/goals' + goals + '_quantiliser_delta.5', 'results', rew+'rewards', runs=runs, color='blue', label='Quantiliser (.5)')\n",
    "    plot_rc_results('reward-corruption/goals' + goals + '_quantiliser_delta.8', 'results', rew+'rewards', runs=runs, color='green', label='Quantiliser (.8)')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc=2)\n",
    "    plt.savefig('reward-corruption/goals' + goals + '_' + rew + '.png', bbox_inches='tight', format='png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward-corruption/goals4_quantiliser_delta.5\t observed rewards 0.919 +- 0.04\t true rewards 0.719 +- 0.36\n"
     ]
    }
   ],
   "source": [
    "# compute average observed and true rewards\n",
    "def round_to_n(x, n):\n",
    "    return round(x, -int(np.floor(np.log10(x))) + (n - 1))\n",
    "\n",
    "def comb(a, b):\n",
    "    return str(round_to_n(a,3)) + ' +- ' + str(round_to_n(b,2))\n",
    "\n",
    "combine = np.vectorize(comb)\n",
    "\n",
    "def average_results(directory,\n",
    "                 filename='results',\n",
    "                 runs=100,\n",
    "                 cycles=1000000):\n",
    "    A = np.zeros((2,runs))\n",
    "    for j in range(runs):\n",
    "        file = open(directory + '/' + filename + '-' + str(j+1) + '.json')\n",
    "        data = json.load(file)\n",
    "        file.close()\n",
    "        A[0,j] = data['Reward Corruption'][0]['rewards'][cycles-1]\n",
    "        A[1,j] = data['Reward Corruption'][0]['true_rewards'][cycles-1]\n",
    "    res = combine(np.mean(A,1), np.std(A, 1))\n",
    "    print (directory + '\\t observed rewards ' + res[0] + '\\t true rewards ' + res[1])\n",
    "\n",
    "average_results('reward-corruption/goals' + goals + '_qlearning')\n",
    "average_results('reward-corruption/goals' + goals + '_softmax')\n",
    "average_results('reward-corruption/goals' + goals + '_quantiliser_delta.2')\n",
    "average_results('reward-corruption/goals' + goals + '_quantiliser_delta.5')\n",
    "average_results('reward-corruption/goals' + goals + '_quantiliser_delta.8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
